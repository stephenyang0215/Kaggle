{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nimport re, string, time\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":2,"outputs":[{"output_type":"stream","text":"/kaggle/input/hindienglish-corpora/Hindi_English_Truncated_Corpus.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"raw = pd.read_csv(r'/kaggle/input/hindienglish-corpora/Hindi_English_Truncated_Corpus.csv')\ncorpus = raw[raw['source']=='ted']\ncorpus","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"       source                                   english_sentence  \\\n0         ted  politicians do not have permission to do what ...   \n1         ted         I'd like to tell you about one such child,   \n3         ted  what we really mean is that they're bad at not...   \n7         ted   And who are we to say, even, that they are wrong   \n13        ted                   So there is some sort of justice   \n...       ...                                                ...   \n127595    ted         is if we want that to become our reality -   \n127597    ted                           Africa has not done bad.   \n127598    ted                                         Thank you.   \n127603    ted                          and put it in our cheeks.   \n127606    ted  They've just won four government contracts to ...   \n\n                                           hindi_sentence  \n0       राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n1       मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n3          हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n7        और हम होते कौन हैं यह कहने भी वाले कि वे गलत हैं  \n13                                       तो वहाँ न्याय है  \n...                                                   ...  \n127595            अगर हम चाहते हैं कि यह वास्तविकता बने -  \n127597                   अफ़्रीका ने कुछ गलती नहीं की है।  \n127598                                          धन्यवाद |  \n127603                    और अपने गालों में डाल लेते हैं।  \n127606  हाल ही में उन्हें सरकारी ठेका मिला है करीब सौ ...  \n\n[39881 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>english_sentence</th>\n      <th>hindi_sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ted</td>\n      <td>politicians do not have permission to do what ...</td>\n      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ted</td>\n      <td>I'd like to tell you about one such child,</td>\n      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ted</td>\n      <td>what we really mean is that they're bad at not...</td>\n      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>ted</td>\n      <td>And who are we to say, even, that they are wrong</td>\n      <td>और हम होते कौन हैं यह कहने भी वाले कि वे गलत हैं</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>ted</td>\n      <td>So there is some sort of justice</td>\n      <td>तो वहाँ न्याय है</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>127595</th>\n      <td>ted</td>\n      <td>is if we want that to become our reality -</td>\n      <td>अगर हम चाहते हैं कि यह वास्तविकता बने -</td>\n    </tr>\n    <tr>\n      <th>127597</th>\n      <td>ted</td>\n      <td>Africa has not done bad.</td>\n      <td>अफ़्रीका ने कुछ गलती नहीं की है।</td>\n    </tr>\n    <tr>\n      <th>127598</th>\n      <td>ted</td>\n      <td>Thank you.</td>\n      <td>धन्यवाद |</td>\n    </tr>\n    <tr>\n      <th>127603</th>\n      <td>ted</td>\n      <td>and put it in our cheeks.</td>\n      <td>और अपने गालों में डाल लेते हैं।</td>\n    </tr>\n    <tr>\n      <th>127606</th>\n      <td>ted</td>\n      <td>They've just won four government contracts to ...</td>\n      <td>हाल ही में उन्हें सरकारी ठेका मिला है करीब सौ ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>39881 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(df, language):\n    df[language] = df[language].apply(lambda x:x.lower().strip())\n    df[language] = df[language].apply(lambda x:re.sub(r\"([?.!,])\", r\" \\1 \", x))\n    if language == 'english_sentence':\n        df[language] = df[language].apply(lambda x: re.sub(r\"[^a-zA-Z?.!,]+\", \" \", x))\n    df[language] = df[language].apply(lambda x:'<start> '+x+' <end>')\n    return df[language]\ncorpus['english_sentence'] = preprocessing(corpus, 'english_sentence')\ncorpus['hindi_sentence'] = preprocessing(corpus, 'hindi_sentence')","execution_count":4,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  This is separate from the ipykernel package so we can avoid doing imports until\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \"\"\"\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  if __name__ == '__main__':\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"dictionary=set()\nfor sentence in corpus['english_sentence'].to_list():\n    for word in sentence.split():\n        dictionary.add(word)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize_seq(corpus_ln):\n    ln_tokenizer = Tokenizer()\n    ln_tokenizer.fit_on_texts(corpus_ln)\n    tensor = ln_tokenizer.texts_to_sequences(corpus_ln)\n    tensor = pad_sequences(tensor, padding='post')\n    return tensor, ln_tokenizer\nen_tensor, en_tokenizer = tokenize_seq(corpus['english_sentence'])\nhin_tensor, hin_tokenizer = tokenize_seq(corpus['hindi_sentence'])","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_train, input_val, target_train, target_val = train_test_split(en_tensor, hin_tensor, test_size=0.2)\nprint(input_train.shape, 'English Training Shape')\nprint(input_val.shape, 'English Validation Shape')\nprint(target_train.shape, 'Indie Training Shape')\nprint(target_val.shape, 'Indie Validation Shape')","execution_count":7,"outputs":[{"output_type":"stream","text":"(31904, 23) English Training Shape\n(7977, 23) English Validation Shape\n(31904, 32) Indie Training Shape\n(7977, 32) Indie Validation Shape\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size=64\nbuffer_size=len(input_train)\nsteps_per_batch=len(input_train)//64\nembed_dim=256\nunits=1024\nen_word_len = len(en_tokenizer.word_index)+1\nhin_word_len = len(hin_tokenizer.word_index)+1\ndataset=tf.data.Dataset.from_tensor_slices((input_train, target_train)).shuffle(buffer_size)\ndataset=dataset.batch(batch_size, drop_remainder=True)\nval_dataset = tf.data.Dataset.from_tensor_slices((input_val, target_val)).batch(batch_size, drop_remainder=True)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exp_input, exp_target = next(iter(dataset))\nprint(exp_input.shape)\nprint(exp_target.shape)","execution_count":9,"outputs":[{"output_type":"stream","text":"(64, 23)\n(64, 32)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Encode(tf.keras.Model):\n    def __init__(self, units, batch, vocab_size, embed_dim):\n        super().__init__()\n        self.units = units\n        self.batch = batch\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_dim)\n        self.gru = tf.keras.layers.GRU(self.units, return_sequences=True, return_state=True, \n                                       recurrent_initializer='glorot_uniform')\n    def call(self, x, hidden):\n        x = self.embedding(x)\n        output, state = self.gru(x, initial_state = hidden)\n        return output, state\n    def initialization(self):\n        return tf.zeros((self.batch, self.units))","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = Encode(units, batch_size, en_word_len, embed_dim)\nexp_hidden = encoder.initialization()\nexp_input_tensor, exp_hidden = encoder(exp_input, exp_hidden)\nprint(exp_input_tensor.shape)\nprint(exp_hidden.shape)","execution_count":11,"outputs":[{"output_type":"stream","text":"(64, 23, 1024)\n(64, 1024)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Attention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super().__init__()\n        self.w1 = tf.keras.layers.Dense(units)\n        self.w2 = tf.keras.layers.Dense(units)\n        self.v = tf.keras.layers.Dense(1)\n    def call(self, encoded_input, state):\n        state = tf.expand_dims(state, 1)\n        score = self.v(tf.nn.tanh(self.w1(encoded_input)+self.w2(state)))\n        attention_weights = tf.nn.softmax(score, axis=1)\n        context_vector = attention_weights*encoded_input\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        return context_vector, attention_weights","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights_func = Attention(units)\nvector, weights = weights_func(exp_input_tensor, exp_hidden)\nprint(vector.shape)\nprint(weights.shape)","execution_count":13,"outputs":[{"output_type":"stream","text":"(64, 1024)\n(64, 23, 1)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Decoder(tf.keras.Model):\n    def __init__(self, units, batch, vocab_size, embed_dim):\n        super().__init__()\n        self.units = units\n        self.batch = batch\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_dim)\n        self.gru = tf.keras.layers.GRU(self.units, return_sequences=True, return_state=True\n                                       , recurrent_initializer='glorot_uniform')\n        self.dense = tf.keras.layers.Dense(vocab_size)\n        self.attention = Attention(self.units)\n    def call(self, x, encoded_input, state):\n        context_vector, weights = self.attention(encoded_input, state)\n        x = self.embedding(x)\n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n        output, state = self.gru(x)\n        output = tf.reshape(output, (-1, output.shape[2]))\n        output = self.dense(output)\n        return output, state, weights","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder = Decoder(units, batch_size, hin_word_len, embed_dim)\noutput, state, weights = decoder(tf.random.uniform((batch_size, 1)), exp_input_tensor, exp_hidden)\nprint(output.shape)\nprint(state.shape)\nprint(weights.shape)","execution_count":15,"outputs":[{"output_type":"stream","text":"(64, 22088)\n(64, 1024)\n(64, 23, 1)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\ndef loss_function(value, output):\n    mask = tf.math.logical_not(tf.math.equal(value, 0))\n    loss = loss_object(value, output)\n    mask = tf.cast(mask, dtype = loss.dtype)\n    loss*=mask\n    return tf.reduce_mean(loss)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef train_step(input, target, hidden):\n    loss=0\n    with tf.GradientTape() as tape:\n        enc_output, enc_hidden = encoder(input, hidden)\n        dec_input = tf.expand_dims([hin_tokenizer.word_index['start']]*batch_size, 1)\n        \n        for i in range(1, target.shape[1]):\n            dec_output, dec_state, _ = decoder(dec_input, enc_output, hidden)\n            loss += loss_function(target[:, i], dec_output)\n            dec_input = tf.expand_dims(target[:, i], 1)\n        \n    batch_loss = loss / int(target.shape[1])\n    var = encoder.trainable_variables+decoder.trainable_variables\n    grad = tape.gradient(loss, var)\n    optimizer.apply_gradients(zip(grad, var))\n    return batch_loss","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 15\nfor epoch in range(epochs):\n    start = time.time()\n    hidden = encoder.initialization()\n    total_loss = 0\n    for (batch, (inputs, targets)) in enumerate(dataset.take(steps_per_batch)):\n        batch_loss = train_step(inputs, targets, hidden)\n        total_loss+=batch_loss\n        \n        if batch % 100 ==0:\n            print('Epoch {} Batch{} Loss {:.4f}'.format(epoch+1, batch, batch_loss.numpy()))\n        \n    print('Epoch {} Loss {:.4f}'.format(epoch+1, total_loss/steps_per_batch))\n    print('Total Computation Time Per Epoch: {} sec\\n'.format(time.time()-start))","execution_count":18,"outputs":[{"output_type":"stream","text":"Epoch 1 Batch0 Loss 2.7986\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-6037de420372>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(corpus):\n    attention = np.zeros(input_val.shape[1], input_train.shape[1])\n    inputs = [en_tokenizer.word_index[i] for i in corpus.split(' ')]\n    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n                                                         maxlen=en_word_len,\n                                                         padding='post')\n    inputs = tf.convert_to_tensor(inputs)\n\n    result = ''\n\n    hidden = [tf.zeros((1, units))]\n    enc_out, enc_hidden = encoder(inputs, hidden)\n    dec_hidden = enc_hidden\n    dec_input = tf.expand_dims([hin_tokenizer.word_index['<start>']], 0)\n    for t in range(max_length_targ):\n        predictions, dec_hidden, attention_weights = decoder(dec_input,dec_hidden,enc_out)\n    attention_weights = tf.reshape(attention_weights, (-1, ))\n    attention_plot[t] = attention_weights.numpy()\n\n    predicted_id = tf.argmax(predictions[0]).numpy()\n\n    result += targ_lang.index_word[predicted_id] + ' '\n\n    if hin_tokenizer.index_word[predicted_id] == '<end>':\n        return result, sentence, attention_plot\n\n    # the predicted ID is fed back into the model\n    dec_input = tf.expand_dims([predicted_id], 0)\n    return result, sentence, attention_plot ","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def translate(corpus):\n    result, corpus, attention_plot = evaluate(corpus)\n    print('Input: %s' % (sentence))\n    print('Predicted translation: {}'.format(result))\n    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw[raw['source']=='tides']","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"       source                                   english_sentence  \\\n5       tides  The then Governor of Kashmir resisted transfer...   \n9       tides  You may want your child to go to a school that...   \n10      tides  Please ensure that you use the appropriate form .   \n14      tides  The first two were found unreliable and the pr...   \n15      tides  They had justified their educational policy of...   \n...       ...                                                ...   \n127599  tides  About 85 per cent of our tea production was di...   \n127600  tides  Certain other tests may be necessary for speci...   \n127601  tides  It is now widening its boundaries and it may i...   \n127604  tides  As for the other derivatives of sulphur , the ...   \n127605  tides  its complicated functioning is defined thus in...   \n\n                                           hindi_sentence  \n5       कश्मीर के तत्कालीन गवर्नर ने इस हस्तांतरण का व...  \n9       हो सकता है कि आप चाहते हों कि आप का नऋर्नमेनटे...  \n10      कृपया यह सुनिश्चित कर लें कि आप सही फॉर्म का प...  \n14      पहले दो को अविश्वसनीय मानकर बाकी पांच मुखबिरों...  \n15      कम संख़्या वाले उच्च एवं मध्यम श्रेणी के लोगों...  \n...                                                   ...  \n127599  हमारे चाय उत्पादन का 85 प्रतिशत सीधे ब्रिटेन क...  \n127600  विशेष स्थितियों में कुछ अन्य परीक्षणों की भी ज...  \n127601  अब यह अपनी सीमाओं को फैला रहा है और हो सकता है...  \n127604  जहां तक गंधक के अन्य उत्पादों का प्रश्न है , द...  \n127605  Zरचना-प्रकिया को उसने एक पहेली में यों बांधा है .  \n\n[50000 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>english_sentence</th>\n      <th>hindi_sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>tides</td>\n      <td>The then Governor of Kashmir resisted transfer...</td>\n      <td>कश्मीर के तत्कालीन गवर्नर ने इस हस्तांतरण का व...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>tides</td>\n      <td>You may want your child to go to a school that...</td>\n      <td>हो सकता है कि आप चाहते हों कि आप का नऋर्नमेनटे...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>tides</td>\n      <td>Please ensure that you use the appropriate form .</td>\n      <td>कृपया यह सुनिश्चित कर लें कि आप सही फॉर्म का प...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>tides</td>\n      <td>The first two were found unreliable and the pr...</td>\n      <td>पहले दो को अविश्वसनीय मानकर बाकी पांच मुखबिरों...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>tides</td>\n      <td>They had justified their educational policy of...</td>\n      <td>कम संख़्या वाले उच्च एवं मध्यम श्रेणी के लोगों...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>127599</th>\n      <td>tides</td>\n      <td>About 85 per cent of our tea production was di...</td>\n      <td>हमारे चाय उत्पादन का 85 प्रतिशत सीधे ब्रिटेन क...</td>\n    </tr>\n    <tr>\n      <th>127600</th>\n      <td>tides</td>\n      <td>Certain other tests may be necessary for speci...</td>\n      <td>विशेष स्थितियों में कुछ अन्य परीक्षणों की भी ज...</td>\n    </tr>\n    <tr>\n      <th>127601</th>\n      <td>tides</td>\n      <td>It is now widening its boundaries and it may i...</td>\n      <td>अब यह अपनी सीमाओं को फैला रहा है और हो सकता है...</td>\n    </tr>\n    <tr>\n      <th>127604</th>\n      <td>tides</td>\n      <td>As for the other derivatives of sulphur , the ...</td>\n      <td>जहां तक गंधक के अन्य उत्पादों का प्रश्न है , द...</td>\n    </tr>\n    <tr>\n      <th>127605</th>\n      <td>tides</td>\n      <td>its complicated functioning is defined thus in...</td>\n      <td>Zरचना-प्रकिया को उसने एक पहेली में यों बांधा है .</td>\n    </tr>\n  </tbody>\n</table>\n<p>50000 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}